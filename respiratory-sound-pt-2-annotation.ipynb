{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respiratoy sound automatic annotation (1D Conv, without spectrograms, no downsampling)\n",
    "Attempt automatic annotation of sound files in the same format as the provided text files.\n",
    "Use the sound data directly without creating spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score, precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "try:\n",
    "    os.environ['KAGGLE_DATA_PROXY_TOKEN']\n",
    "except KeyError:\n",
    "    dir_out = \"./\"\n",
    "    dir_files = \"Respiratory_Sound_Database/Respiratory_Sound_Database/\"\n",
    "    dir_annot_csv = \"./\"\n",
    "    fname_demo = \"./demographic_info.txt\"\n",
    "else:\n",
    "    dir_out = \"/kaggle/working/\"\n",
    "    dir_files = \"/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/\"\n",
    "    dir_annot_csv = \"/kaggle/input/recording-annotations/\"\n",
    "    fname_demo = \"/kaggle/input/respiratory-sound-database/\" + \"demographic_info.txt\"\n",
    "    \n",
    "fname_diag = dir_files + \"patient_diagnosis.csv\"\n",
    "fname_annot = dir_annot_csv + \"rec_annotation.csv\"\n",
    "tfrecord_wavs = dir_out + \"wavs.tfrecord\"\n",
    "tfrecord_wavs = \"./tfrecords2/wavs_0.tfrecord\"\n",
    "tfrecord_annot = dir_out + \"annot.tfrecord\"\n",
    "dir_audio = dir_files + \"audio_and_txt_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "group_pat_num = \"([0-9]{3})\"\n",
    "group_rec_index = \"([0-9][a-z][0-9])\"\n",
    "group_chest_loc = \"(Tc|Al|Ar|Pl|Pr|Ll|Lr)\"\n",
    "group_acc_modes = \"(sc|mc)\"\n",
    "group_equipments = \"(AKGC417L|LittC2SE|Litt3200|Meditron)\"\n",
    "\n",
    "regex_info = re.compile(\"_\".join([group_pat_num, group_rec_index, group_chest_loc, group_acc_modes, group_equipments]))\n",
    "\n",
    "top = os.getcwd()\n",
    "os.chdir(dir_audio)\n",
    "fnames = glob.glob(\"*.wav\")\n",
    "\n",
    "l_wav_rec = []\n",
    "dict_wav_rec = {}\n",
    "min_len = np.inf\n",
    "max_len = 0\n",
    "\n",
    "for fname in fnames:\n",
    "    match_info = regex_info.match(fname)\n",
    "    pat_num = int(match_info.group(1))\n",
    "    rec_index = match_info.group(2)\n",
    "    chest_loc = match_info.group(3)\n",
    "    acc_mode = match_info.group(4)\n",
    "    equipment = match_info.group(5)\n",
    "    \n",
    "    wav_content = sf.read(fname)[0]\n",
    "    l_wav_rec.append([pat_num, rec_index, chest_loc, wav_content])\n",
    "    dict_wav_rec[(pat_num, rec_index, chest_loc)] = wav_content\n",
    "    \n",
    "    if len(wav_content) > max_len:\n",
    "        max_len = len(wav_content)\n",
    "        # for getting the corresponding annotation below\n",
    "        max_patnum = pat_num\n",
    "        max_recindex = rec_index\n",
    "        max_chestloc = chest_loc\n",
    "    \n",
    "    if len(wav_content) < min_len:\n",
    "        min_len = len(wav_content)\n",
    "        # for getting the corresponding annotation below\n",
    "        min_patnum = pat_num\n",
    "        min_recindex = rec_index\n",
    "        min_chestloc = chest_loc\n",
    "\n",
    "os.chdir(top)\n",
    "\n",
    "# pad all recordings to same length\n",
    "for i in range(len(l_wav_rec)):\n",
    "    if len(l_wav_rec[i][3]) < max_len:\n",
    "        padding = [0] * ( max_len - len(l_wav_rec[i][3]) )\n",
    "        l_wav_rec[i][3] = np.append(l_wav_rec[i][3], padding)\n",
    "\n",
    "# pad all recordings to multiple of length of shortest recording\n",
    "# for i in range(len(l_wav_rec)):\n",
    "#     if len(l_wav_rec[i][3]) % min_len != 0:\n",
    "#         padding = [0] * ( min_len - len(l_wav_rec[i][3]) % min_len)\n",
    "#         l_wav_rec[i][3] = np.append(l_wav_rec[i][3], padding)\n",
    "\n",
    "l_wav_rec.sort(key=lambda subl: (subl[0], subl[1], subl[2]))\n",
    "wav_cols = [\"Patient number\", \"Recording index\", \"Chest location\", \"WAV\"]\n",
    "df_wav_rec = pd.DataFrame(l_wav_rec, columns=wav_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotation corresponding to smallest wave file\n",
    "df_annot = pd.read_csv(fname_annot,\n",
    "                       dtype = {\"Patient number\" : \"string\"}).set_index([\"Patient number\", \"Recording index\", \"Chest location\"])\n",
    "df_annot = df_annot.sort_index()\n",
    "# df_min_annot = df_annot.loc[min_patnum, min_recindex, min_chestloc]\n",
    "# df_max_annot = df_annot.loc[max_patnum, max_recindex, max_chestloc]\n",
    "\n",
    "max_len_annot = 0\n",
    "max_ix = None\n",
    "\n",
    "for ix in df_annot.index:\n",
    "    cur_annot = df_annot.loc[ix, [\"Cycle start\", \"Cycle end\", \"Crackles\", \"Wheezes\"]]\n",
    "    # pad additional \"dummy\" rows to maximum length annotation\n",
    "    if len(cur_annot) > max_len_annot:\n",
    "        max_len_annot = len(cur_annot)\n",
    "        df_max_annot = cur_annot\n",
    "#         max_ix = ix\n",
    "\n",
    "# construct CNN target feature maps\n",
    "# target_feature_maps = np.empty((0,max_len_annot,4), dtype=\"float32\")\n",
    "# for ix in df_annot.index:\n",
    "#     cur_annot = df_annot.loc[ix, [\"Cycle start\", \"Cycle end\", \"Crackles\", \"Wheezes\"]].to_numpy()\n",
    "#     # pad additional \"dummy\" rows to maximum length annotation\n",
    "#     if len(cur_annot) < max_len_annot:\n",
    "#         pad_len = max_len_annot - len(cur_annot)\n",
    "#         # what's the best way of padding?\n",
    "#         padding = np.array([[-1, -1, 0, 0]] * pad_len)\n",
    "#         cur_annot = np.vstack((cur_annot, padding))\n",
    "#     feature_map = cur_annot[np.newaxis, :]\n",
    "#     target_feature_maps = np.append(target_feature_maps, feature_map, axis=0)\n",
    "\n",
    "# construct CNN target feature maps\n",
    "target_feature_maps = np.empty((0,max_len_annot,4), dtype=\"float16\")\n",
    "# target_feature_maps = {}\n",
    "for ix in df_annot.index.unique().sort_values():\n",
    "    cur_annot = df_annot.loc[ix, [\"Cycle start\", \"Cycle end\", \"Crackles\", \"Wheezes\"]].to_numpy()\n",
    "    # pad additional \"dummy\" rows to maximum length annotation\n",
    "    if len(cur_annot) < max_len_annot:\n",
    "        pad_len = max_len_annot - len(cur_annot)\n",
    "        # what's the best way of padding?\n",
    "        padding = np.array([[-1, -1, 0, 0]] * pad_len)\n",
    "        cur_annot = np.vstack((cur_annot, padding))\n",
    "    feature_map = cur_annot[np.newaxis, :]\n",
    "    target_feature_maps = np.append(target_feature_maps, feature_map, axis=0)\n",
    "#     target_feature_maps[ix] = feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val_ix in range(len(l_wav_rec[0][3])):\n",
    "#     feature = Feature(\n",
    "#         float_list = FloatList(value=([l_wav_rec[0][3][val_ix]]))\n",
    "#     )\n",
    "    \n",
    "#     feats = [feature]\n",
    "    \n",
    "#     feat_list = FeatureList(feature = feats)\n",
    "    \n",
    "#     feat_lists_dict[\"{} {} {} {}\".format(l_wav_rec[0][0], l_wav_rec[0][1], l_wav_rec[0][2], val_ix)] = feat_list\n",
    "    \n",
    "#     sequence_description[\"{} {} {} {}\".format(l_wav_rec[0][0], l_wav_rec[0][1], l_wav_rec[0][2], val_ix)] = tf.io.FixedLenSequenceFeature([1], tf.float32)\n",
    "\n",
    "# tfr_options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "# tfr_options = tf.io.TFRecordOptions()\n",
    "# f = tf.io.TFRecordWriter(tfrecord_wavs, tfr_options)\n",
    "\n",
    "# for rec in l_wav_rec:\n",
    "    \n",
    "#     feat_patnum = Feature(\n",
    "#         int64_list = Int64List(value=[rec[0]])\n",
    "#     )\n",
    "\n",
    "#     feat_recix = Feature(\n",
    "#         bytes_list = BytesList(value=[bytes(rec[1], \"ascii\")])\n",
    "#     )\n",
    "\n",
    "#     feat_chestloc = Feature(\n",
    "#         bytes_list = BytesList(value=[bytes(rec[2], \"ascii\")])\n",
    "#     )\n",
    "\n",
    "#     features_context = Features(\n",
    "#         feature = {\n",
    "#             \"patient number\" : feat_patnum,\n",
    "#             \"recording index\" : feat_recix,\n",
    "#             \"chest location\" : feat_chestloc\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     feat_list = [ Feature(\n",
    "#             float_list = FloatList(value=(rec[3]))\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     feat_lists_dict = {}    \n",
    "#     feat_lists_dict[\"WAV\"] = FeatureList(feature = feat_list)\n",
    "#     feat_lists = FeatureLists(feature_list = feat_lists_dict)\n",
    "    \n",
    "#     feat_rec_str = Feature(\n",
    "#         bytes_list = BytesList(value=[bytes(\"{}_{}_{}\".format(rec[0], rec[1], rec[2]), \"ascii\")])\n",
    "#     )\n",
    "#     features_context = Features(\n",
    "#         feature = {\n",
    "#             \"Recording string\" : feat_rec_str\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "    \n",
    "\n",
    "#     se = SequenceExample(\n",
    "#         context = features_context,\n",
    "#         feature_lists = feat_lists\n",
    "#     )\n",
    "\n",
    "\n",
    "#     f.write(se.SerializeToString())\n",
    "    \n",
    "    \n",
    "#     reshaped_tensor = tf.reshape(tf.constant(rec[3], dtype=tf.float32), shape=[-1, 1])\n",
    "#     f.write(tf.io.serialize_tensor(reshaped_tensor).numpy())\n",
    "    \n",
    "\n",
    "# f.close()\n",
    "\n",
    "# free RAM\n",
    "# del l_wav_rec\n",
    "\n",
    "\n",
    "\n",
    "# with tf.io.TFRecordWriter(tfrecord_annot, tfr_options) as f:\n",
    "#     f.write(tf.io.serialize_tensor(tf.constant(target_feature_maps, dtype=tf.float32)).numpy())\n",
    "\n",
    "# del target_feature_maps\n",
    "    \n",
    "# feature_context = {\n",
    "#     \"patient number\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "#     \"recording index\" : tf.io.FixedLenFeature([], tf.string),\n",
    "#     \"chest location\" : tf.io.FixedLenFeature([], tf.string)\n",
    "#     }\n",
    "\n",
    "# feature_context = {\n",
    "#     \"Recording string\" : tf.io.FixedLenFeature([], tf.string)\n",
    "# }\n",
    "\n",
    "\n",
    "# sequence_description = {\n",
    "#     \"WAV\" : tf.io.FixedLenSequenceFeature([max_len], tf.float32)\n",
    "# }\n",
    "\n",
    "# for serialized in tf.data.TFRecordDataset([fname_tfrecord]).batch(10):\n",
    "#     parsed = tf.io.parse_sequence_example(serialized, feature_context, sequence_description)\n",
    "    \n",
    "# for serialized in tf.data.TFRecordDataset([fname_tfrecord], compression_type=\"GZIP\").batch(10):\n",
    "#     for s in serialized:\n",
    "#         parsed_tensor = tf.io.parse_tensor(s, tf.float32)\n",
    "#         print(parsed_tensor)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# print(tf.reshape(tf.constant(parsed[1]['WAV'], dtype=tf.float32), shape=[-1, 1]))\n",
    "# print(parsed)\n",
    "# print(l_wav_rec[2][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for serialized in tf.data.TFRecordDataset([tfrecord_annot]).batch(2):\n",
    "#     for s in serialized:\n",
    "#         parsed_tensor = tf.io.parse_tensor(s, tf.float32)\n",
    "#         print(parsed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: 110 1p1 Al.  Can't parse serialized Example.\n\t [[{{node ParseExample/ParseExampleV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   2101\u001b[0m       \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2102\u001b[1;33m       \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2103\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2609\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2610\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2611\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Key: 110 1p1 Al.  Can't parse serialized Example.\n\t [[{{node ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8289a025c8c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#         print(parsed_tensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasetFromTFRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-8289a025c8c7>\u001b[0m in \u001b[0;36mdatasetFromTFRecord\u001b[1;34m(fpath, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#     return parsed_wavs.batch(batch_size).prefetch(1), target_data.batch(batch_size).prefetch(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_wavs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mixs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   2103\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m       \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2105\u001b[1;33m       \u001b[0mexecutor_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Key: 110 1p1 Al.  Can't parse serialized Example.\n\t [[{{node ParseExample/ParseExampleV2}}]]"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"unique_rec_ids.csv\")\n",
    "# feat_parse_desc = dict(zip(df[\"0\"], [tf.io.VarLenFeature(tf.float32)] * len(df)))\n",
    "feat_parse_desc = dict(zip(df[\"0\"], [tf.io.FixedLenFeature([], dtype = tf.float32, default_value = 0)] * len(df)))\n",
    "\n",
    "def datasetFromTFRecord(fpath=tfrecord_wavs, batch_size=10):\n",
    "    \n",
    "    tfr_wav = tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\")\n",
    "#     tfr_annot = tf.data.TFRecordDataset([tfrecord_annot])\n",
    "#     serialized = tfr_dataset.batch(batch_size)\n",
    "    \n",
    "    parsed_wavs = tfr_wav.map(lambda ser : tf.io.parse_example(ser, feat_parse_desc))\n",
    "#     parsed_annot = tfr_annot.map(lambda ser : tf.io.parse_tensor(ser, tf.float32)).unbatch()\n",
    "    \n",
    "#     batch_target = copy.deepcopy(target_feature_maps[:batch_size])\n",
    "#     batch_target = copy.deepcopy(target_feature_maps[:3])\n",
    "#     target_data = tf.data.Dataset.from_tensors(batch_target)\n",
    "#     target_data = tf.expand_dims(tf.constant(batch_target), 0)\n",
    "\n",
    "    \n",
    "#     wavs_batch = tf.constant(list(parsed_wavs.batch(batch_size).prefetch(1).as_numpy_iterator()))\n",
    "    \n",
    "#     zipped = tf.data.Dataset.zip((parsed_wavs, parsed_annot))\n",
    "    \n",
    "#     data = tf.data.Dataset.from_tensor_slices((parsed_wavs, parsed_annot))\n",
    "    \n",
    "#     np.delete(target_feature_maps, batch_size)\n",
    "    \n",
    "#     data = tf.data.Dataset.from_tensor_slices((wavs_batch, target_data))\n",
    "\n",
    "\n",
    "#     return parsed_wavs.batch(batch_size).prefetch(1), target_data.batch(batch_size).prefetch(1)\n",
    "\n",
    "    data_dict = list(parsed_wavs.batch(batch_size))[0]\n",
    "\n",
    "    ixs = [ tuple(key.split()) for key in data_dict.keys() ]\n",
    "\n",
    "    y_arr = np.empty((0,max_len_annot,4), dtype=\"float32\")\n",
    "    \n",
    "    for ix in ixs:\n",
    "        feat_map = target_feature_maps[ix]\n",
    "        np.append(y_arr, feature_map, axis=0)\n",
    "    \n",
    "#     y = target_feature_maps.loc[ixs]\n",
    "    \n",
    "    y = tf.data.Dataset.from_tensor_slices(y_arr)\n",
    "    \n",
    "    X = [sparse.values for sparse in list(data_dict.values()) ]\n",
    "    \n",
    "    X = tf.stack(X, axis = 0)\n",
    "    X = StandardScaler(X)\n",
    "    \n",
    "    X = tf.data.Dataset.from_tensor_slices(X)\n",
    "    \n",
    "    zipped = tf.data.Dataset.zip((X, y))\n",
    "\n",
    "#     return parsed_wavs.batch(batch_size).prefetch(1)\n",
    "    return zipped.batch(batch_size).prefetch(1)\n",
    "#     return data.batch(batch_size).prefetch(1)\n",
    "    \n",
    "#     return data.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for serialized in tf.data.TFRecordDataset([fname_tfrecord], compression_type=\"GZIP\").batch(10):\n",
    "# for serialized in tf.data.TFRecordDataset([tfrecord_annot]).batch(2):\n",
    "\n",
    "# for ser in tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\").batch(10):\n",
    "#     for tensor in serialized:\n",
    "#         parsed_tensor = tf.io.parse_example(tensor, feat_parse_desc)\n",
    "#         print(parsed_tensor)\n",
    "\n",
    "for ser in datasetFromTFRecord():\n",
    "    print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 50, 882001), dtype=float32, numpy=\n",
      "array([[[ 9.4543457e-02,  9.4879150e-02,  9.5092773e-02, ...,\n",
      "          8.1695557e-02,  8.1634521e-02,  0.0000000e+00],\n",
      "        [-3.6773562e-02, -3.6895633e-02, -3.6956668e-02, ...,\n",
      "         -4.2907596e-02, -4.3151736e-02,  0.0000000e+00],\n",
      "        [ 3.8452148e-03,  3.8757324e-03,  3.9367676e-03, ...,\n",
      "         -1.5136719e-02, -1.5502930e-02,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 3.0517578e-05,  0.0000000e+00, -1.5258789e-04, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-3.1378174e-01, -3.1723022e-01, -3.1970215e-01, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-3.0838013e-01, -3.0792236e-01, -3.0792236e-01, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32)>, <tf.Tensor: shape=(1, 920, 33, 4), dtype=float64, numpy=\n",
      "array([[[[ 0.036,  0.579,  0.   ,  0.   ],\n",
      "         [ 0.579,  2.45 ,  0.   ,  0.   ],\n",
      "         [ 2.45 ,  3.893,  0.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]],\n",
      "\n",
      "        [[ 0.036,  1.264,  0.   ,  0.   ],\n",
      "         [ 1.264,  3.422,  0.   ,  0.   ],\n",
      "         [ 3.422,  5.55 ,  0.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]],\n",
      "\n",
      "        [[ 0.264,  1.736,  0.   ,  0.   ],\n",
      "         [ 1.736,  3.293,  0.   ,  0.   ],\n",
      "         [ 3.293,  5.307,  0.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.022,  0.979,  0.   ,  0.   ],\n",
      "         [ 0.979,  2.864,  0.   ,  0.   ],\n",
      "         [ 2.864,  4.921,  1.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]],\n",
      "\n",
      "        [[ 0.107,  2.207,  0.   ,  0.   ],\n",
      "         [ 2.207,  4.064,  1.   ,  0.   ],\n",
      "         [ 4.064,  5.893,  1.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]],\n",
      "\n",
      "        [[ 0.036,  1.207,  0.   ,  0.   ],\n",
      "         [ 1.207,  3.55 ,  0.   ,  0.   ],\n",
      "         [ 3.55 ,  5.75 ,  1.   ,  0.   ],\n",
      "         ...,\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ],\n",
      "         [-1.   , -1.   ,  0.   ,  0.   ]]]])>)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"unique_rec_ids.csv\")\n",
    "# feat_parse_desc = dict(zip(df[\"0\"], [tf.io.VarLenFeature(tf.float32)] * len(df)))\n",
    "feat_parse_desc = dict(zip(df[\"0\"], [tf.io.FixedLenFeature([], dtype = tf.float32, default_value = 0)] * len(df)))\n",
    "\n",
    "def datasetFromTFRecord(fpath=tfrecord_wavs, batch_size=10):\n",
    "    \n",
    "    tfr_wav = tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\")\n",
    "#     tfr_annot = tf.data.TFRecordDataset([tfrecord_annot])\n",
    "#     serialized = tfr_dataset.batch(batch_size)\n",
    "    \n",
    "#     parsed_wavs = tfr_wav.map(lambda ser : tf.ensure_shape(tf.io.parse_tensor(ser, tf.float32), (920, 882000)))\n",
    "    parsed_wavs = tfr_wav.map(lambda ser : tf.io.parse_tensor(ser, tf.float32))\n",
    "#     wavs_reshape = parsed_wavs.map(lambda tensor :  tf.reshape( tensor, shape=[-1, 1]))\n",
    "#     parsed_annot = tfr_annot.map(lambda ser : tf.io.parse_tensor(ser, tf.float32)).unbatch()\n",
    "    \n",
    "#     batch_target = copy.deepcopy(target_feature_maps[:batch_size])\n",
    "#     batch_target = copy.deepcopy(target_feature_maps[:3])\n",
    "#     target_data = tf.data.Dataset.from_tensors(batch_target)\n",
    "#     target_data = tf.expand_dims(tf.constant(batch_target), 0)\n",
    "\n",
    "    y_data = tf.data.Dataset.from_tensors(target_feature_maps)\n",
    "    \n",
    "#     wavs_batch = tf.constant(list(parsed_wavs.batch(batch_size).prefetch(1).as_numpy_iterator()))\n",
    "    \n",
    "    zipped = tf.data.Dataset.zip((parsed_wavs, y_data))\n",
    "#     zipped = tf.data.Dataset.zip((wavs_reshape, y_data))\n",
    "    \n",
    "#     data = tf.data.Dataset.from_tensor_slices((parsed_wavs, parsed_annot))\n",
    "    \n",
    "#     np.delete(target_feature_maps, batch_size)\n",
    "    \n",
    "#     data = tf.data.Dataset.from_tensor_slices((wavs_batch, target_data))\n",
    "\n",
    "\n",
    "#     return parsed_wavs.batch(batch_size).prefetch(1), target_data.batch(batch_size).prefetch(1)\n",
    "\n",
    "#     data_dict = list(parsed_wavs.batch(batch_size))[0]\n",
    "\n",
    "#     ixs = [ tuple(key.split()) for key in data_dict.keys() ]\n",
    "\n",
    "#     y_arr = np.empty((0,max_len_annot,4), dtype=\"float32\")\n",
    "    \n",
    "#     for ix in ixs:\n",
    "#         feat_map = target_feature_maps[ix]\n",
    "#         np.append(y_arr, feature_map, axis=0)\n",
    "    \n",
    "# #     y = target_feature_maps.loc[ixs]\n",
    "    \n",
    "#     y = tf.data.Dataset.from_tensor_slices(y_arr)\n",
    "    \n",
    "#     X = [sparse.values for sparse in list(data_dict.values()) ]\n",
    "    \n",
    "#     X = tf.stack(X, axis = 0)\n",
    "#     X = StandardScaler(X)\n",
    "    \n",
    "#     X = tf.data.Dataset.from_tensor_slices(X)\n",
    "    \n",
    "#     zipped = tf.data.Dataset.zip((X, y))\n",
    "\n",
    "#     return parsed_wavs.batch(batch_size).prefetch(1)\n",
    "\n",
    "    return zipped.batch(batch_size).prefetch(1)\n",
    "#     return parsed_wavs.batch(batch_size).prefetch(1), y_data.batch(batch_size).prefetch(1)\n",
    "#     return data.batch(batch_size).prefetch(1)\n",
    "    \n",
    "#     return data.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for serialized in tf.data.TFRecordDataset([fname_tfrecord], compression_type=\"GZIP\").batch(10):\n",
    "# for serialized in tf.data.TFRecordDataset([tfrecord_annot]).batch(2):\n",
    "\n",
    "# for ser in tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\").batch(10):\n",
    "#     for tensor in serialized:\n",
    "#         parsed_tensor = tf.io.parse_example(tensor, feat_parse_desc)\n",
    "#         print(parsed_tensor)\n",
    "\n",
    "# for ser in tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\").batch(10):\n",
    "#     for tensor in ser:\n",
    "#         parsed_tensor = tf.io.parse_tensor(tensor, tf.float32)\n",
    "#         print(parsed_tensor)\n",
    "\n",
    "data = tf.data.TFRecordDataset([tfrecord_wavs], compression_type=\"GZIP\")\n",
    "parsed_wavs = data.map(lambda ser : tf.io.parse_tensor(ser, tf.float32))\n",
    "# wavs_reshape = parsed_wavs.map(lambda tensor :  tf.reshape( tensor, shape=[-1, 1]))\n",
    "# for e in wavs_reshape:\n",
    "# for e in parsed_wavs:\n",
    "#     print(e.shape)\n",
    "\n",
    "y_data = tf.data.Dataset.from_tensors(target_feature_maps)\n",
    "\n",
    "# for e in y_data:\n",
    "#     print(e.shape)\n",
    "\n",
    "zipped = tf.data.Dataset.zip((parsed_wavs, y_data))\n",
    "\n",
    "# for e in zipped.batch(10):\n",
    "#     print(e[0].shape)\n",
    "    \n",
    "for ser in datasetFromTFRecord():\n",
    "    print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1195 mean_squared_error\n        return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10398 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 36 and 33 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](1DCNN/conv_output/Relu, Cast)' with input shapes: [?,36,4], [?,920,33,4].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6b7e530b0e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetFromTFRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m# model.build(input_shape=[None, max_len, 1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# model.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1195 mean_squared_error\n        return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10398 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 36 and 33 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](1DCNN/conv_output/Relu, Cast)' with input shapes: [?,36,4], [?,920,33,4].\n"
     ]
    }
   ],
   "source": [
    "# x1 = np.array([[df_wav_rec.loc[0, \"WAV\"].astype(\"float32\")]])\n",
    "# x1 = np.array([df_wav_rec.loc[0, \"WAV\"].astype(\"float32\")]).reshape(-1,1)[np.newaxis,:,:]\n",
    "# conv = keras.layers.Conv1D(filters=4,kernel_size=5, strides=2, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "cnn_strides = [1,2,4]\n",
    "fmap_size = 822001\n",
    "\n",
    "model = keras.models.Sequential(name=\"1DCNN\")\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((-1, 1), input_shape=(882001,)))\n",
    "\n",
    "for i in range(len(cnn_strides)):\n",
    "    if i == 0:\n",
    "        layer_name = \"conv_input\"\n",
    "    else:\n",
    "        layer_name = \"conv_{}\".format(i)\n",
    "    model.add(keras.layers.Conv1D(filters = 64 / (2 ** i) , kernel_size= 5 * (2 ** i), strides=cnn_strides[i], padding=\"same\", activation=\"relu\", name=layer_name))\n",
    "    fmap_size //= cnn_strides[i]\n",
    "\n",
    "# calculate this in order to get the last stride number that results in a output feature map of\n",
    "# size of longest rec. annotation\n",
    "last_strides = fmap_size // max_len_annot + 1\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters = 4 , kernel_size= 5 * (2 ** (i+1)), strides=last_strides, padding=\"same\", activation=\"relu\", name=\"conv_output\"))\n",
    "\n",
    "model.compile(loss=\"mse\", metrics=[\"accuracy\"], optimizer=\"sgd\")\n",
    "\n",
    "\n",
    "# conv(tf.expand_dims(parsed_tensor, 0))\n",
    "# it = iter(tf.data.TFRecordDataset([fname_tfrecord]).batch(1))\n",
    "# serialized = next(it).numpy()\n",
    "# parsed_tensor = tf.io.parse_tensor(serialized[0], tf.float32)\n",
    "# model.fit(tf.expand_dims(parsed_tensor, 0), target_feature_maps[0][np.newaxis, :, :])\n",
    "\n",
    "train_set = datasetFromTFRecord()\n",
    "model.fit(train_set)\n",
    "# model.build(input_shape=[None, max_len, 1])\n",
    "# model.summary()\n",
    "\n",
    "# for e in train_set.batch(1).as_numpy_iterator():\n",
    "#     print(e)\n",
    "#     model.fit(e[0], e[1])\n",
    "#     out = conv(e[0])\n",
    "#     print(conv(out))\n",
    "\n",
    "    \n",
    "\n",
    "# list(train_set.as_numpy_iterator())\n",
    "\n",
    "    \n",
    "# model.train_on_batch()\n",
    "# target_feature_maps[0][np.newaxis, :, :].shape\n",
    "\n",
    "# model.fit(l_wav_rec, target_feature_maps)\n",
    "# model.fit(x2, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
